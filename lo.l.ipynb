{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas_by_sentence(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences = {}\n",
    "\n",
    "    def extract_pos(morph):\n",
    "        return morph[0] if morph else None\n",
    "\n",
    "    for sentence in root.findall('.//s'):\n",
    "        sentence_number = sentence.attrib.get('n', 'unknown')\n",
    "        sentences[sentence_number] = []\n",
    "        \n",
    "        for token in sentence.findall('.//t'):\n",
    "            word_info = {'word_form': '', 'morph': '', 'lemmas_pos': []}\n",
    "\n",
    "            word_form = token.find('f')\n",
    "            if word_form is not None:\n",
    "                word_info['word_form'] = word_form.text\n",
    "          \n",
    "            word_info['morph'] = token.attrib.get('o', '')\n",
    "            \n",
    "            lemma = token.find('l')\n",
    "            if lemma is not None:\n",
    "                for l1 in lemma.findall('l1'):\n",
    "                    pos = extract_pos(l1.attrib.get('o', ''))\n",
    "                    if pos:\n",
    "                        word_info['lemmas_pos'].append((l1.text, pos))\n",
    "\n",
    "                for l2 in lemma.findall('l2'):\n",
    "                    pos = extract_pos(l2.attrib.get('o', ''))\n",
    "                    if pos:\n",
    "                        word_info['lemmas_pos'].append((l2.text, pos))\n",
    "            \n",
    "            sentences[sentence_number].append(word_info)\n",
    "    \n",
    "    split_path = path.split(\".\")\n",
    "    \n",
    "    filename = f\"{'.'.join(split_path[:3])}.json\"\n",
    "        \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentences, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_with_lemma(lemma, paths):\n",
    "    sents = []\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            corpus = json.load(file)\n",
    "\n",
    "            for sent_id in enumerate(corpus):\n",
    "\n",
    "                for word_dic in corpus[f\"{sent_id}\"]:\n",
    "\n",
    "                    for lemma_pos in word_dic[\"lemma_pos\"]:\n",
    "                        corpus_lemma = lemma_pos[0]\n",
    "                    \n",
    "                        if corpus_lemma is not None and lemma == corpus_lemma:\n",
    "                            word_forms = [dic[\"word_form\"] for dic in corpus[f\"{sent_id}\"]]\n",
    "                            sent = \" \".join(word_forms) \n",
    "                            sents.append(sent)\n",
    "                            break\n",
    "                \n",
    "                if len(sents) == 4:\n",
    "                    break\n",
    "\n",
    "    return sents"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
